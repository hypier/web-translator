# Intel Xeonä¸Šçš„RAGç¤ºä¾‹
æ­¤æ¨¡æ¿åœ¨IntelÂ® XeonÂ®å¯æ‰©å±•å¤„ç†å™¨ä¸Šä½¿ç”¨Chromaå’Œæ–‡æœ¬ç”Ÿæˆæ¨ç†æ‰§è¡ŒRAGã€‚  
IntelÂ® XeonÂ®å¯æ‰©å±•å¤„ç†å™¨å…·æœ‰å†…ç½®åŠ é€Ÿå™¨ï¼Œæä¾›æ›´é«˜çš„æ¯æ ¸å¿ƒæ€§èƒ½å’Œæ— ä¸ä¼¦æ¯”çš„AIæ€§èƒ½ï¼Œé…å¤‡å…ˆè¿›çš„å®‰å…¨æŠ€æœ¯ä»¥æ»¡è¶³æœ€è‹›åˆ»çš„å·¥ä½œè´Ÿè½½éœ€æ±‚â€”â€”åŒæ—¶æä¾›æœ€å¤§çš„äº‘é€‰æ‹©å’Œåº”ç”¨ç¨‹åºå¯ç§»æ¤æ€§ï¼Œè¯·æŸ¥çœ‹[IntelÂ® XeonÂ®å¯æ‰©å±•å¤„ç†å™¨](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html)ã€‚

## ç¯å¢ƒè®¾ç½®
è¦åœ¨ IntelÂ® XeonÂ® Scalable å¤„ç†å™¨ä¸Šä½¿ç”¨ [ğŸ¤— text-generation-inference](https://github.com/huggingface/text-generation-inference)ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

### åœ¨Intel XeonæœåŠ¡å™¨ä¸Šå¯åŠ¨æœ¬åœ°æœåŠ¡å™¨å®ä¾‹ï¼š
```bash
model=Intel/neural-chat-7b-v3-3
volume=$PWD/data # ä¸Dockerå®¹å™¨å…±äº«å·ï¼Œä»¥é¿å…æ¯æ¬¡è¿è¡Œæ—¶ä¸‹è½½æƒé‡

docker run --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.4 --model-id $model
```

å¯¹äºè¯¸å¦‚`LLAMA-2`çš„å—é™æ¨¡å‹ï¼Œæ‚¨éœ€è¦åœ¨ä¸Šè¿°docker runå‘½ä»¤ä¸­ä¼ é€’ -e HUGGING_FACE_HUB_TOKEN=\<token\>ï¼Œå¹¶ä½¿ç”¨æœ‰æ•ˆçš„Hugging Face Hubè¯»å–ä»¤ç‰Œã€‚

è¯·è®¿é—®æ­¤é“¾æ¥ [huggingface token](https://huggingface.co/docs/hub/security-tokens) è·å–è®¿é—®ä»¤ç‰Œï¼Œå¹¶ä½¿ç”¨è¯¥ä»¤ç‰Œå¯¼å‡º`HUGGINGFACEHUB_API_TOKEN`ç¯å¢ƒå˜é‡ã€‚

```bash
export HUGGINGFACEHUB_API_TOKEN=<token> 
```

å‘é€è¯·æ±‚ä»¥æ£€æŸ¥ç«¯ç‚¹æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```bash
curl localhost:8080/generate -X POST -d '{"inputs":"Which NFL team won the Super Bowl in the 2010 season?","parameters":{"max_new_tokens":128, "do_sample": true}}'   -H 'Content-Type: application/json'
```

æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ [text-generation-inference](https://github.com/huggingface/text-generation-inference)ã€‚

## å¡«å……æ•°æ®

å¦‚æœæ‚¨æƒ³ç”¨ä¸€äº›ç¤ºä¾‹æ•°æ®å¡«å……æ•°æ®åº“ï¼Œå¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```shell
poetry install
poetry run python ingest.py
```

è¯¥è„šæœ¬å¤„ç†å¹¶å­˜å‚¨æ¥è‡ªEdgar 10kç”³æŠ¥æ•°æ®çš„Nike `nke-10k-2023.pdf`çš„éƒ¨åˆ†å†…å®¹åˆ°Chromaæ•°æ®åº“ä¸­ã€‚

## ç”¨æ³•

è¦ä½¿ç”¨æ­¤åŒ…ï¼Œæ‚¨é¦–å…ˆéœ€è¦å®‰è£… LangChain CLIï¼š

```shell
pip install -U langchain-cli
```

è¦åˆ›å»ºä¸€ä¸ªæ–°çš„ LangChain é¡¹ç›®å¹¶å°†æ­¤åŒ…ä½œä¸ºå”¯ä¸€åŒ…å®‰è£…ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œï¼š

```shell
langchain app new my-app --package intel-rag-xeon
```

å¦‚æœæ‚¨æƒ³å°†å…¶æ·»åŠ åˆ°ç°æœ‰é¡¹ç›®ä¸­ï¼Œåªéœ€è¿è¡Œï¼š

```shell
langchain app add intel-rag-xeon
```

å¹¶å°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ°æ‚¨çš„ `server.py` æ–‡ä»¶ä¸­ï¼š
```python
from intel_rag_xeon import chain as xeon_rag_chain

add_routes(app, xeon_rag_chain, path="/intel-rag-xeon")
```

ï¼ˆå¯é€‰ï¼‰ç°åœ¨è®©æˆ‘ä»¬é…ç½® LangSmithã€‚LangSmith å°†å¸®åŠ©æˆ‘ä»¬è·Ÿè¸ªã€ç›‘æ§å’Œè°ƒè¯• LangChain åº”ç”¨ç¨‹åºã€‚æ‚¨å¯ä»¥åœ¨ [è¿™é‡Œ](https://smith.langchain.com/) æ³¨å†Œ LangSmithã€‚å¦‚æœæ‚¨æ²¡æœ‰è®¿é—®æƒé™ï¼Œå¯ä»¥è·³è¿‡æ­¤éƒ¨åˆ†ã€‚

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>
export LANGCHAIN_PROJECT=<your-project>  # å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤ä¸º "default"
```

å¦‚æœæ‚¨åœ¨æ­¤ç›®å½•ä¸­ï¼Œåˆ™å¯ä»¥ç›´æ¥å¯åŠ¨ä¸€ä¸ª LangServe å®ä¾‹ï¼š

```shell
langchain serve
```

è¿™å°†å¯åŠ¨ FastAPI åº”ç”¨ç¨‹åºï¼ŒæœåŠ¡å™¨åœ¨æœ¬åœ°è¿è¡Œï¼Œåœ°å€ä¸º 
[http://localhost:8000](http://localhost:8000)

æˆ‘ä»¬å¯ä»¥åœ¨ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) æŸ¥çœ‹æ‰€æœ‰æ¨¡æ¿
æˆ‘ä»¬å¯ä»¥åœ¨ [http://127.0.0.1:8000/intel-rag-xeon/playground](http://127.0.0.1:8000/intel-rag-xeon/playground) è®¿é—®æ¸¸ä¹åœº

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»£ç è®¿é—®æ¨¡æ¿ï¼š

```python
from langserve.client import RemoteRunnable

runnable = RemoteRunnable("http://localhost:8000/intel-rag-xeon")
```