# é¢„æµ‹ä¿æŠ¤

æœ¬é¡µé¢ä»‹ç»å¦‚ä½•åœ¨ LangChain ä¸­ä½¿ç”¨é¢„æµ‹ä¿æŠ¤ç”Ÿæ€ç³»ç»Ÿã€‚
å®ƒåˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šå®‰è£…å’Œè®¾ç½®ï¼Œä»¥åŠå¯¹ç‰¹å®šé¢„æµ‹ä¿æŠ¤åŒ…è£…å™¨çš„å¼•ç”¨ã€‚

## å®‰è£…å’Œè®¾ç½®
- ä½¿ç”¨ `pip install predictionguard` å®‰è£… Python SDK
- è·å– Prediction Guard è®¿é—®ä»¤ç‰Œï¼ˆå¦‚ [è¿™é‡Œ](https://docs.predictionguard.com/) æ‰€è¿°ï¼‰å¹¶å°†å…¶è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ (`PREDICTIONGUARD_TOKEN`)

## LLM Wrapper

å­˜åœ¨ä¸€ä¸ªé¢„æµ‹ä¿æŠ¤ LLM åŒ…è£…å™¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®å®ƒï¼š
```python
from langchain_community.llms import PredictionGuard
```

åœ¨åˆå§‹åŒ– LLM æ—¶ï¼Œæ‚¨å¯ä»¥å°†é¢„æµ‹ä¿æŠ¤æ¨¡å‹çš„åç§°ä½œä¸ºå‚æ•°æä¾›ï¼š
```python
pgllm = PredictionGuard(model="MPT-7B-Instruct")
```

æ‚¨è¿˜å¯ä»¥ç›´æ¥å°†è®¿é—®ä»¤ç‰Œä½œä¸ºå‚æ•°æä¾›ï¼š
```python
pgllm = PredictionGuard(model="MPT-7B-Instruct", token="<your access token>")
```

æœ€åï¼Œæ‚¨å¯ä»¥æä¾›ä¸€ä¸ªâ€œè¾“å‡ºâ€å‚æ•°ï¼Œç”¨äºç»“æ„åŒ–/æ§åˆ¶ LLM çš„è¾“å‡ºï¼š
```python
pgllm = PredictionGuard(model="MPT-7B-Instruct", output={"type": "boolean"})
```

## ç¤ºä¾‹ç”¨æ³•

å—æ§æˆ–å—ä¿æŠ¤çš„ LLM åŒ…è£…å™¨çš„åŸºæœ¬ç”¨æ³•ï¼š
```python
import os

import predictionguard as pg
from langchain_community.llms import PredictionGuard
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain

# æ‚¨çš„ Prediction Guard API å¯†é’¥ã€‚è¯·åœ¨ predictionguard.com è·å–ä¸€ä¸ª
os.environ["PREDICTIONGUARD_TOKEN"] = "<your Prediction Guard access token>"

# å®šä¹‰ä¸€ä¸ªæç¤ºæ¨¡æ¿
template = """æ ¹æ®ä¸Šä¸‹æ–‡å›åº”ä»¥ä¸‹æŸ¥è¯¢ã€‚

ä¸Šä¸‹æ–‡ï¼šæ¯æ¡è¯„è®ºã€ç§ä¿¡ + é‚®ä»¶å»ºè®®éƒ½å¼•å¯¼æˆ‘ä»¬å‘å¸ƒè¿™ä¸ªä»¤äººå…´å¥‹çš„å…¬å‘Šï¼ğŸ‰ æˆ‘ä»¬æ­£å¼å¢åŠ äº†ä¸¤ä¸ªæ–°çš„èœ¡çƒ›è®¢é˜…ç›’é€‰é¡¹ï¼ğŸ“¦
ç‹¬å®¶èœ¡çƒ›ç›’ - $80 
æ¯æœˆèœ¡çƒ›ç›’ - $45 (æ–°ï¼)
æœ¬æœˆé¦™å‘³ç›’ - $28 (æ–°ï¼)
å‰å¾€æ•…äº‹è·å–æ¯ä¸ªç›’å­çš„æ‰€æœ‰è¯¦æƒ…ï¼ğŸ‘† é™„åŠ ä¼˜æƒ ï¼šä½¿ç”¨ä»£ç  50OFF äº«å—é¦–ä¸ªç›’å­ 50% çš„æŠ˜æ‰£ï¼ğŸ‰

æŸ¥è¯¢ï¼š{query}

ç»“æœï¼š"""
prompt = PromptTemplate.from_template(template)

# é€šè¿‡â€œä¿æŠ¤â€æˆ–æ§åˆ¶ LLM çš„è¾“å‡ºã€‚è¯·å‚é˜… 
# Prediction Guard æ–‡æ¡£ (https://docs.predictionguard.com) äº†è§£å¦‚ä½• 
# ä½¿ç”¨æ•´æ•°ã€æµ®ç‚¹æ•°ã€å¸ƒå°”å€¼ã€JSON å’Œå…¶ä»–ç±»å‹åŠç»“æ„æ§åˆ¶è¾“å‡ºã€‚
pgllm = PredictionGuard(model="MPT-7B-Instruct", 
                        output={
                                "type": "categorical",
                                "categories": [
                                    "äº§å“å…¬å‘Š", 
                                    "é“æ­‰", 
                                    "å…³ç³»"
                                    ]
                                })
pgllm(prompt.format(query="è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„å¸–å­ï¼Ÿ"))
```

ä½¿ç”¨ Prediction Guard åŒ…è£…å™¨çš„åŸºæœ¬ LLM é“¾æ¥ï¼š
```python
import os

from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.llms import PredictionGuard

# å¯é€‰ï¼Œæ·»åŠ æ‚¨çš„ OpenAI API å¯†é’¥ã€‚æ­¤é¡¹ä¸ºå¯é€‰ï¼Œå› ä¸º Prediction Guard å…è®¸
# æ‚¨è®¿é—®æ‰€æœ‰æœ€æ–°çš„å¼€æ”¾è®¿é—®æ¨¡å‹ï¼ˆè¯·å‚è§ https://docs.predictionguard.comï¼‰
os.environ["OPENAI_API_KEY"] = "<your OpenAI api key>"

# æ‚¨çš„ Prediction Guard API å¯†é’¥ã€‚è¯·åœ¨ predictionguard.com è·å–ä¸€ä¸ª
os.environ["PREDICTIONGUARD_TOKEN"] = "<your Prediction Guard access token>"

pgllm = PredictionGuard(model="OpenAI-gpt-3.5-turbo-instruct")

template = """é—®é¢˜ï¼š{question}

ç­”æ¡ˆï¼šè®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒã€‚"""
prompt = PromptTemplate.from_template(template)
llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)

question = "è´¾æ–¯æ±€Â·æ¯”ä¼¯å‡ºç”Ÿé‚£å¹´ï¼Œå“ªæ”¯ NFL çƒé˜Ÿèµ¢å¾—äº†è¶…çº§ç¢—ï¼Ÿ"

llm_chain.predict(question=question)
```