# Intel

>[Optimum Intel](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#optimum-intel) 是 🤗 Transformers 和 Diffusers 库与 Intel 提供的不同工具和库之间的接口，以加速在 Intel 架构上的端到端管道。

>[Intel® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers?tab=readme-ov-file#intel-extension-for-transformers) (ITREX) 是一个创新工具包，旨在以最佳性能加速各种 Intel 平台上基于 Transformer 的模型的 GenAI/LLM，包括 Intel Gaudi2、Intel CPU 和 Intel GPU。

本页面介绍如何将 optimum-intel 和 ITREX 与 LangChain 一起使用。

## Optimum-intel

与 [optimum-intel](https://github.com/huggingface/optimum-intel.git) 和 [IPEX](https://github.com/intel/intel-extension-for-pytorch) 相关的所有功能。

### 安装

使用 optimum-intel 和 ipex 安装：

```bash
pip install optimum[neural-compressor]
pip install intel_extension_for_pytorch
```

请按照以下指定的安装说明进行操作：

* 按照 [这里](https://github.com/huggingface/optimum-intel) 所示安装 optimum-intel。
* 按照 [这里](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.2.0%2Bcpu) 所示安装 IPEX。

### 嵌入模型

请参阅 [用法示例](/docs/integrations/text_embedding/optimum_intel)。  
我们还在食谱目录中提供了完整的教程笔记本 "rag_with_quantized_embeddings.ipynb"，用于在 RAG 管道中使用嵌入器。

```python
from langchain_community.embeddings import QuantizedBiEncoderEmbeddings
```

## Intel® Extension for Transformers (ITREX)
(ITREX) 是一个创新工具包，用于加速基于 Transformer 的模型在 Intel 平台上的运行，特别是在第 4 代 Intel Xeon 可扩展处理器 Sapphire Rapids（代号 Sapphire Rapids）上表现卓越。

量化是一个通过使用更少的位数来表示这些权重，从而降低其精度的过程。仅权重量化专注于对神经网络的权重进行量化，同时保持其他组件（如激活）在其原始精度下。

随着大型语言模型（LLMs）的日益普及，对新型和改进的量化方法的需求不断增加，这些方法能够满足现代架构的计算需求，同时保持准确性。与 [普通量化](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/quantization.md)（如 W8A8）相比，仅权重量化可能是平衡性能和准确性的更好折衷，因为我们将在下面看到，部署 LLM 的瓶颈是内存带宽，而通常仅权重量化可以导致更好的准确性。

在这里，我们将介绍 ITREX 中的嵌入模型和仅权重量化，用于 Transformers 大型语言模型。仅权重量化是一种用于深度学习的技术，用于减少神经网络的内存和计算需求。在深度神经网络的上下文中，模型参数，也称为权重，通常使用浮点数表示，这可能会消耗大量内存并需要大量计算资源。

所有与 [intel-extension-for-transformers](https://github.com/intel/intel-extension-for-transformers) 相关的功能。

### 安装

安装 intel-extension-for-transformers。有关系统要求和其他安装提示，请参阅 [安装指南](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/installation.md)

```bash
pip install intel-extension-for-transformers
```
安装其他所需的包。

```bash
pip install -U torch onnx accelerate datasets
```

### 嵌入模型

查看[使用示例](/docs/integrations/text_embedding/itrex)。

```python
from langchain_community.embeddings import QuantizedBgeEmbeddings
```

### 仅权重量化与 ITREX

请参阅 [使用示例](/docs/integrations/llms/weight_only_quantization)。

## 配置参数细节

以下是 `WeightOnlyQuantConfig` 类的详细信息。

#### weight_dtype (string): 权重数据类型，默认值为 "nf4"。
我们支持将权重量化为以下数据类型以进行存储（WeightOnlyQuantConfig 中的 weight_dtype）：
* **int8**: 使用 8 位数据类型。
* **int4_fullrange**: 使用 int4 范围的 -8 值，与正常的 int4 范围 [-7,7] 相比。
* **int4_clip**: 裁剪并保留 int4 范围内的值，将其他值设为零。
* **nf4**: 使用规范化的 4 位浮点数据类型。
* **fp4_e2m1**: 使用常规的 4 位浮点数据类型。"e2" 表示 2 位用于指数，"m1" 表示 1 位用于尾数。

#### compute_dtype (string): 计算数据类型，默认值为 "fp32"。
虽然这些技术将权重存储为 4 位或 8 位，但计算仍然发生在 float32、bfloat16 或 int8（WeightOnlyQuantConfig 中的 compute_dtype）：
* **fp32**: 使用 float32 数据类型进行计算。
* **bf16**: 使用 bfloat16 数据类型进行计算。
* **int8**: 使用 8 位数据类型进行计算。

#### llm_int8_skip_modules (list of module's name): 跳过量化的模块，默认值为 None。
这是一个要跳过量化的模块列表。

#### scale_dtype (string): 缩放数据类型，默认值为 "fp32"。
目前仅支持 "fp32"（float32）。

#### mse_range (boolean): 是否从范围 [0.805, 1.0, 0.005] 中搜索最佳裁剪范围，默认值为 False。
#### use_double_quant (boolean): 是否量化缩放，默认值为 False。
尚不支持。
#### double_quant_dtype (string): 保留用于双重量化。
#### double_quant_scale_dtype (string): 保留用于双重量化。
#### group_size (int): 量化时的组大小。
#### scheme (string): 权重量化的格式。默认值为 "sym"。
* **sym**: 对称。
* **asym**: 非对称。
#### algorithm (string): 用于提高准确性的算法。默认值为 "RTN"。
* **RTN**: 最近邻舍入 (RTN) 是一种我们可以非常直观理解的量化方法。
* **AWQ**: 仅保护 1% 的显著权重可以大大减少量化误差。显著权重通道是通过观察每个通道的激活和权重分布来选择的。显著权重在量化前在乘以一个大缩放因子后也会被量化以进行保留。
* **TEQ**: 一种可训练的等效变换，在仅权重量化中保留 FP32 精度。