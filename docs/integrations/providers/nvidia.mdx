# NVIDIA
`langchain-nvidia-ai-endpoints` 包含与 NVIDIA NIM 推理微服务上模型构建应用的 LangChain 集成。NIM 支持来自社区和 NVIDIA 的聊天、嵌入和重新排序模型等各个领域的模型。这些模型经过 NVIDIA 优化，以在 NVIDIA 加速基础设施上提供最佳性能，并作为 NIM 部署，NIM 是一种易于使用的预构建容器，可以通过单个命令在任何地方部署，适用于 NVIDIA 加速基础设施。

可以在 [NVIDIA API catalog](https://build.nvidia.com/) 上测试 NVIDIA 托管的 NIM 部署。测试后，NIM 可以使用 NVIDIA AI Enterprise 许可证从 NVIDIA 的 API 目录导出，并在本地或云中运行，使企业拥有其知识产权和 AI 应用的所有权和完全控制权。

NIM 以每个模型为基础打包为容器镜像，并通过 NVIDIA NGC 目录作为 NGC 容器镜像分发。NIM 的核心提供了易于使用、一致且熟悉的 API，用于在 AI 模型上运行推理。

以下是如何使用一些与文本生成和嵌入模型相关的常见功能的示例。

## 安装

```python
pip install -U --quiet langchain-nvidia-ai-endpoints
```

## 设置

**开始使用：**

1. 创建一个免费的[NVIDIA](https://build.nvidia.com/)账户，该网站托管NVIDIA AI基础模型。

2. 点击您选择的模型。

3. 在输入选项下选择Python标签，然后点击`获取API密钥`。接着点击`生成密钥`。

4. 复制并保存生成的密钥为NVIDIA_API_KEY。之后，您应该能够访问端点。

```python
import getpass
import os

if not os.environ.get("NVIDIA_API_KEY", "").startswith("nvapi-"):
    nvidia_api_key = getpass.getpass("Enter your NVIDIA API key: ")
    assert nvidia_api_key.startswith("nvapi-"), f"{nvidia_api_key[:5]}... is not a valid key"
    os.environ["NVIDIA_API_KEY"] = nvidia_api_key
```

## 使用 NVIDIA API 目录

```python
from langchain_nvidia_ai_endpoints import ChatNVIDIA

llm = ChatNVIDIA(model="mistralai/mixtral-8x22b-instruct-v0.1")
result = llm.invoke("Write a ballad about LangChain.")
print(result.content)
```

使用 API，您可以查询 NVIDIA API 目录中可用的实时端点，以便从 DGX 托管的云计算环境中快速获取结果。所有模型都是源可访问的，并且可以使用 NVIDIA AI Enterprise 中的一部分 NVIDIA NIM 部署到您自己的计算集群，详细信息请参见下一节 [使用 NVIDIA NIM](##working-with-nvidia-nims)。

## 使用 NVIDIA NIMs
准备好部署后，您可以使用 NVIDIA NIM 自托管模型——该功能包含在 NVIDIA AI Enterprise 软件许可中——并可以在任何地方运行它们，从而拥有您自定义的所有权，并完全控制您的知识产权（IP）和 AI 应用程序。

[了解更多关于 NIMs 的信息](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)

```python
from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank

# connect to a chat NIM running at localhost:8000, specifying a model
llm = ChatNVIDIA(base_url="http://localhost:8000/v1", model="meta/llama3-8b-instruct")

# connect to an embedding NIM running at localhost:8080
embedder = NVIDIAEmbeddings(base_url="http://localhost:8080/v1")

# connect to a reranking NIM running at localhost:2016
ranker = NVIDIARerank(base_url="http://localhost:2016/v1")
```

## 使用 NVIDIA AI Foundation 端点

一系列 NVIDIA AI Foundation 模型可以通过熟悉的 API 在 LangChain 中直接使用。

支持的活动模型可以在 [API Catalog](https://build.nvidia.com/) 中找到。

**以下示例可能对您入门有所帮助：**
- **[`ChatNVIDIA` 模型](/docs/integrations/chat/nvidia_ai_endpoints)。**
- **[`NVIDIAEmbeddings` 模型用于 RAG 工作流](/docs/integrations/text_embedding/nvidia_ai_endpoints)。**