# Ollama

>[Ollama](https://ollama.com/) 允许您在本地运行开源的大型语言模型，例如 LLaMA2。
>
>`Ollama` 将模型权重、配置和数据打包成一个由 Modelfile 定义的单一包。 
>它优化了设置和配置细节，包括 GPU 使用。
>有关支持的模型和模型变体的完整列表，请参见 [Ollama 模型库](https://ollama.ai/library)。

有关如何将 `Ollama` 与 LangChain 一起使用的更多详细信息，请参见 [本指南](/docs/how_to/local_llms)。

## 安装和设置

按照 [这些说明](https://github.com/ollama/ollama?tab=readme-ov-file#ollama) 设置并运行本地 Ollama 实例。

## LLM

```python
from langchain_community.llms import Ollama
```

查看笔记本示例 [这里](/docs/integrations/llms/ollama)。

## 聊天模型

### Chat Ollama

```python
from langchain_community.chat_models import ChatOllama
```

请查看笔记本示例 [这里](/docs/integrations/chat/ollama)。

### Ollama 函数

```python
from langchain_experimental.llms.ollama_functions import OllamaFunctions
```

请参阅笔记本示例 [这里](/docs/integrations/chat/ollama_functions)。

## 嵌入模型

```python
from langchain_community.embeddings import OllamaEmbeddings
```

请参阅笔记本示例 [这里](/docs/integrations/text_embedding/ollama)。