---
custom_edit_url: https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/extraction_examples.ipynb
---

# å¦‚ä½•åœ¨æå–æ—¶ä½¿ç”¨å‚è€ƒç¤ºä¾‹

é€šè¿‡å‘ LLM æä¾›å‚è€ƒç¤ºä¾‹ï¼Œæå–çš„è´¨é‡é€šå¸¸å¯ä»¥å¾—åˆ°æ”¹å–„ã€‚

æ•°æ®æå–æ—¨åœ¨ç”Ÿæˆæ–‡æœ¬åŠå…¶ä»–éç»“æ„åŒ–æˆ–åŠç»“æ„åŒ–æ ¼å¼ä¸­æ‰€å‘ç°ä¿¡æ¯çš„ç»“æ„åŒ–è¡¨ç¤ºã€‚æ­¤ä¸Šä¸‹æ–‡ä¸­é€šå¸¸ä½¿ç”¨ [Tool-calling](/docs/concepts#functiontool-calling) LLM ç‰¹æ€§ã€‚æœ¬æŒ‡å—æ¼”ç¤ºå¦‚ä½•æ„å»ºå·¥å…·è°ƒç”¨çš„å°‘é‡ç¤ºä¾‹ï¼Œä»¥å¸®åŠ©å¼•å¯¼æå–åŠç±»ä¼¼åº”ç”¨çš„è¡Œä¸ºã€‚

:::tip
è™½ç„¶æœ¬æŒ‡å—ä¸“æ³¨äºå¦‚ä½•ä½¿ç”¨å·¥å…·è°ƒç”¨æ¨¡å‹çš„ç¤ºä¾‹ï¼Œä½†è¯¥æŠ€æœ¯é€šå¸¸é€‚ç”¨ï¼Œå¹¶ä¸”ä¹Ÿé€‚ç”¨äºåŸºäº JSON æˆ–æç¤ºçš„æŠ€æœ¯ã€‚
:::

LangChain åœ¨åŒ…å«å·¥å…·è°ƒç”¨çš„ LLM æ¶ˆæ¯ä¸Šå®ç°äº† [tool-call å±æ€§](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage.tool_calls)ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ [å·¥å…·è°ƒç”¨æŒ‡å—](/docs/how_to/tool_calling)ã€‚ä¸ºäº†æ„å»ºæ•°æ®æå–çš„å‚è€ƒç¤ºä¾‹ï¼Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªåŒ…å«ä»¥ä¸‹åºåˆ—çš„èŠå¤©å†å²ï¼š

- [HumanMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html) åŒ…å«ç¤ºä¾‹è¾“å…¥ï¼›
- [AIMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html) åŒ…å«ç¤ºä¾‹å·¥å…·è°ƒç”¨ï¼›
- [ToolMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.tool.ToolMessage.html) åŒ…å«ç¤ºä¾‹å·¥å…·è¾“å‡ºã€‚

LangChain é‡‡ç”¨è¿™ç§çº¦å®šåœ¨ä¸åŒ LLM æ¨¡å‹æä¾›è€…ä¹‹é—´ç»“æ„åŒ–å·¥å…·è°ƒç”¨çš„å¯¹è¯ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªåŒ…å«è¿™äº›æ¶ˆæ¯å ä½ç¬¦çš„æç¤ºæ¨¡æ¿ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰æç¤ºä»¥æä¾›æŒ‡ç¤ºå’Œä»»ä½•é¢å¤–ä¸Šä¸‹æ–‡ã€‚
# 1) æ‚¨å¯ä»¥åœ¨æç¤ºæ¨¡æ¿ä¸­æ·»åŠ ç¤ºä¾‹ä»¥æé«˜æå–è´¨é‡
# 2) å¼•å…¥é¢å¤–å‚æ•°ä»¥è€ƒè™‘ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼ŒåŒ…å«æå–æ–‡æœ¬çš„æ–‡æ¡£çš„å…ƒæ•°æ®ã€‚ï¼‰
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are an expert extraction algorithm. "
            "Only extract relevant information from the text. "
            "If you do not know the value of an attribute asked "
            "to extract, return null for the attribute's value.",
        ),
        # â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
        MessagesPlaceholder("examples"),  # <-- ç¤ºä¾‹ï¼
        # â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘
        ("human", "{text}"),
    ]
)
```

æµ‹è¯•æ¨¡æ¿ï¼š

```python
from langchain_core.messages import (
    HumanMessage,
)

prompt.invoke(
    {"text": "this is some text", "examples": [HumanMessage(content="testing 1 2 3")]}
)
```

```output
ChatPromptValue(messages=[SystemMessage(content="You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value."), HumanMessage(content='testing 1 2 3'), HumanMessage(content='this is some text')])
```

## å®šä¹‰æ¨¡å¼

è®©æˆ‘ä»¬é‡ç”¨æ¥è‡ª [æå–æ•™ç¨‹](/docs/tutorials/extraction) çš„äººå‘˜æ¨¡å¼ã€‚

```python
from typing import List, Optional

from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI


class Person(BaseModel):
    """å…³äºä¸€ä¸ªäººçš„ä¿¡æ¯ã€‚"""

    # ^ å®ä½“ Person çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
    # è¯¥æ–‡æ¡£å­—ç¬¦ä¸²ä½œä¸ºæ¨¡å¼ Person çš„æè¿°å‘é€ç»™ LLMï¼Œ
    # å¹¶ä¸”å¯ä»¥å¸®åŠ©æ”¹å–„æå–ç»“æœã€‚

    # æ³¨æ„ï¼š
    # 1. æ¯ä¸ªå­—æ®µéƒ½æ˜¯ `optional` -- è¿™å…è®¸æ¨¡å‹æ‹’ç»æå–å®ƒï¼
    # 2. æ¯ä¸ªå­—æ®µéƒ½æœ‰ä¸€ä¸ª `description` -- è¿™ä¸ªæè¿°è¢« LLM ä½¿ç”¨ã€‚
    # æœ‰ä¸€ä¸ªå¥½çš„æè¿°å¯ä»¥å¸®åŠ©æ”¹å–„æå–ç»“æœã€‚
    name: Optional[str] = Field(..., description="è¿™ä¸ªäººçš„åå­—")
    hair_color: Optional[str] = Field(
        ..., description="å¦‚æœå·²çŸ¥ï¼Œè¿™ä¸ªäººçš„å¤´å‘é¢œè‰²"
    )
    height_in_meters: Optional[str] = Field(..., description="ä»¥ç±³ä¸ºå•ä½çš„é«˜åº¦")


class Data(BaseModel):
    """å…³äºäººä»¬çš„æå–æ•°æ®ã€‚"""

    # åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æå–å¤šä¸ªå®ä½“ã€‚
    people: List[Person]
```

## å®šä¹‰å‚è€ƒç¤ºä¾‹

ç¤ºä¾‹å¯ä»¥å®šä¹‰ä¸ºè¾“å…¥-è¾“å‡ºå¯¹çš„åˆ—è¡¨ã€‚

æ¯ä¸ªç¤ºä¾‹åŒ…å«ä¸€ä¸ªç¤ºä¾‹ `input` æ–‡æœ¬å’Œä¸€ä¸ªç¤ºä¾‹ `output`ï¼Œæ˜¾ç¤ºåº”ä»æ–‡æœ¬ä¸­æå–çš„å†…å®¹ã€‚

:::important
è¿™éƒ¨åˆ†å†…å®¹æ¯”è¾ƒç»†èŠ‚ï¼Œå¯ä»¥é€‰æ‹©è·³è¿‡ã€‚

ç¤ºä¾‹çš„æ ¼å¼éœ€è¦ä¸æ‰€ä½¿ç”¨çš„ API åŒ¹é…ï¼ˆä¾‹å¦‚ï¼Œå·¥å…·è°ƒç”¨æˆ– JSON æ¨¡å¼ç­‰ï¼‰ã€‚

åœ¨è¿™é‡Œï¼Œæ ¼å¼åŒ–çš„ç¤ºä¾‹å°†åŒ¹é…å·¥å…·è°ƒç”¨ API çš„é¢„æœŸæ ¼å¼ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨è¿™ä¸ªã€‚
:::


```python
import uuid
from typing import Dict, List, TypedDict

from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.pydantic_v1 import BaseModel, Field


class Example(TypedDict):
    """ä¸€ä¸ªç¤ºä¾‹çš„è¡¨ç¤ºï¼Œç”±æ–‡æœ¬è¾“å…¥å’Œé¢„æœŸçš„å·¥å…·è°ƒç”¨ç»„æˆã€‚

    å¯¹äºæå–ï¼Œå·¥å…·è°ƒç”¨è¡¨ç¤ºä¸º pydantic æ¨¡å‹çš„å®ä¾‹ã€‚
    """

    input: str  # è¿™æ˜¯ç¤ºä¾‹æ–‡æœ¬
    tool_calls: List[BaseModel]  # åº”è¯¥æå–çš„ pydantic æ¨¡å‹å®ä¾‹


def tool_example_to_messages(example: Example) -> List[BaseMessage]:
    """å°†ç¤ºä¾‹è½¬æ¢ä¸ºå¯ä»¥è¾“å…¥ LLM çš„æ¶ˆæ¯åˆ—è¡¨ã€‚

    è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªé€‚é…å™¨ï¼Œå°†æˆ‘ä»¬çš„ç¤ºä¾‹è½¬æ¢ä¸ºå¯ä»¥è¾“å…¥èŠå¤©æ¨¡å‹çš„æ¶ˆæ¯åˆ—è¡¨ã€‚

    æ¯ä¸ªç¤ºä¾‹çš„æ¶ˆæ¯åˆ—è¡¨å¯¹åº”äºï¼š

    1) HumanMessage: åŒ…å«åº”æå–å†…å®¹çš„æ–‡æœ¬ã€‚
    2) AIMessage: åŒ…å«ä»æ¨¡å‹æå–çš„ä¿¡æ¯
    3) ToolMessage: å‘æ¨¡å‹ç¡®è®¤æ¨¡å‹æ­£ç¡®è¯·æ±‚äº†å·¥å…·ã€‚

    ToolMessage æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºæŸäº›èŠå¤©æ¨¡å‹æ˜¯é’ˆå¯¹ä»£ç†è¿›è¡Œè¶…ä¼˜åŒ–çš„
    è€Œä¸æ˜¯é’ˆå¯¹æå–ç”¨ä¾‹ã€‚
    """
    messages: List[BaseMessage] = [HumanMessage(content=example["input"])]
    tool_calls = []
    for tool_call in example["tool_calls"]:
        tool_calls.append(
            {
                "id": str(uuid.uuid4()),
                "args": tool_call.dict(),
                # ç›®å‰å‡½æ•°çš„åç§°å¯¹åº”äº
                # pydantic æ¨¡å‹çš„åç§°
                # è¿™åœ¨ API ä¸­æ˜¯éšå¼çš„ï¼Œ
                # å°†éšç€æ—¶é—´çš„æ¨ç§»è€Œæ”¹è¿›ã€‚
                "name": tool_call.__class__.__name__,
            },
        )
    messages.append(AIMessage(content="", tool_calls=tool_calls))
    tool_outputs = example.get("tool_outputs") or [
        "æ‚¨å·²æ­£ç¡®è°ƒç”¨æ­¤å·¥å…·ã€‚"
    ] * len(tool_calls)
    for output, tool_call in zip(tool_outputs, tool_calls):
        messages.append(ToolMessage(content=output, tool_call_id=tool_call["id"]))
    return messages
```

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å®šä¹‰ç¤ºä¾‹ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼ã€‚


```python
examples = [
    (
        "æµ·æ´‹æµ©ç€šè€Œè”šè“ã€‚æ·±åº¦è¶…è¿‡ 20,000 è‹±å°ºã€‚é‡Œé¢æœ‰å¾ˆå¤šé±¼ã€‚",
        Data(people=[]),
    ),
    (
        "è²å¥¥å¨œä»æ³•å›½è¿œé“è€Œæ¥ï¼Œå‰å¾€è¥¿ç­ç‰™ã€‚",
        Data(people=[Person(name="Fiona", height_in_meters=None, hair_color=None)]),
    ),
]


messages = []

for text, tool_call in examples:
    messages.extend(
        tool_example_to_messages({"input": text, "tool_calls": [tool_call]})
    )
```

è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æç¤º


```python
example_prompt = prompt.invoke({"text": "è¿™æ˜¯ä¸€æ®µæ–‡æœ¬", "examples": messages})

for message in example_prompt.messages:
    print(f"{message.type}: {message}")
```
```output
system: content="æ‚¨æ˜¯ä¸€ä¸ªä¸“å®¶çº§çš„æå–ç®—æ³•ã€‚åªæå–æ–‡æœ¬ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚å¦‚æœæ‚¨ä¸çŸ¥é“è¦æå–çš„å±æ€§çš„å€¼ï¼Œè¯·è¿”å›è¯¥å±æ€§å€¼çš„ nullã€‚"
human: content="æµ·æ´‹æµ©ç€šè€Œè”šè“ã€‚æ·±åº¦è¶…è¿‡ 20,000 è‹±å°ºã€‚é‡Œé¢æœ‰å¾ˆå¤šé±¼ã€‚"
ai: content='' tool_calls=[{'name': 'Person', 'args': {'name': None, 'hair_color': None, 'height_in_meters': None}, 'id': 'b843ba77-4c9c-48ef-92a4-54e534f24521'}]
tool: content='æ‚¨å·²æ­£ç¡®è°ƒç”¨æ­¤å·¥å…·ã€‚' tool_call_id='b843ba77-4c9c-48ef-92a4-54e534f24521'
human: content='è²å¥¥å¨œä»æ³•å›½è¿œé“è€Œæ¥ï¼Œå‰å¾€è¥¿ç­ç‰™ã€‚'
ai: content='' tool_calls=[{'name': 'Person', 'args': {'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}, 'id': '46f00d6b-50e5-4482-9406-b07bb10340f6'}]
tool: content='æ‚¨å·²æ­£ç¡®è°ƒç”¨æ­¤å·¥å…·ã€‚' tool_call_id='46f00d6b-50e5-4482-9406-b07bb10340f6'
human: content='è¿™æ˜¯ä¸€æ®µæ–‡æœ¬'
```

## åˆ›å»ºæå–å™¨

è®©æˆ‘ä»¬é€‰æ‹©ä¸€ä¸ª LLMã€‚å› ä¸ºæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨å·¥å…·è°ƒç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ”¯æŒå·¥å…·è°ƒç”¨åŠŸèƒ½çš„æ¨¡å‹ã€‚è¯·å‚è§ [æ­¤è¡¨](/docs/integrations/chat) ä»¥è·å–å¯ç”¨çš„ LLMã€‚

import ChatModelTabs from "@theme/ChatModelTabs";

<ChatModelTabs
  customVarName="llm"
  openaiParams={`model="gpt-4-0125-preview", temperature=0`}
/>

æŒ‰ç…§ [æå–æ•™ç¨‹](/docs/tutorials/extraction)ï¼Œæˆ‘ä»¬ä½¿ç”¨ `.with_structured_output` æ–¹æ³•æ ¹æ®æ‰€éœ€çš„æ¶æ„æ¥ç»“æ„åŒ–æ¨¡å‹è¾“å‡ºï¼š


```python
runnable = prompt | llm.with_structured_output(
    schema=Data,
    method="function_calling",
    include_raw=False,
)
```

## æ²¡æœ‰ç¤ºä¾‹ ğŸ˜¿

è¯·æ³¨æ„ï¼Œå³ä½¿æ˜¯èƒ½åŠ›å¼ºå¤§çš„æ¨¡å‹ä¹Ÿå¯èƒ½åœ¨**éå¸¸ç®€å•**çš„æµ‹è¯•ç”¨ä¾‹ä¸­å¤±è´¥ï¼


```python
for _ in range(5):
    text = "The solar system is large, but earth has only 1 moon."
    print(runnable.invoke({"text": text, "examples": []}))
```
```output
people=[Person(name='earth', hair_color='null', height_in_meters='null')]
people=[Person(name='earth', hair_color='null', height_in_meters='null')]
people=[]
people=[Person(name='earth', hair_color='null', height_in_meters='null')]
people=[]
```

## å¸¦ç¤ºä¾‹ ğŸ˜»

å‚è€ƒç¤ºä¾‹æœ‰åŠ©äºä¿®å¤æ•…éšœï¼


```python
for _ in range(5):
    text = "The solar system is large, but earth has only 1 moon."
    print(runnable.invoke({"text": text, "examples": messages}))
```
```output
people=[]
people=[]
people=[]
people=[]
people=[]
```
è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ [Langsmith trace](https://smith.langchain.com/public/4c436bc2-a1ce-440b-82f5-093947542e40/r) ä¸­çœ‹åˆ°å‡ æ¬¡ç¤ºä¾‹ä½œä¸ºå·¥å…·è°ƒç”¨ã€‚

å¹¶ä¸”æˆ‘ä»¬åœ¨ä¸€ä¸ªæ­£æ ·æœ¬ä¸Šä¿æŒæ€§èƒ½ï¼š


```python
runnable.invoke(
    {
        "text": "My name is Harrison. My hair is black.",
        "examples": messages,
    }
)
```



```output
Data(people=[Person(name='Harrison', hair_color='black', height_in_meters=None)])
```