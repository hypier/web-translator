---
custom_edit_url: https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/configure.ipynb
sidebar_position: 7
keywords: [ConfigurableField, configurable_fields, ConfigurableAlternatives, configurable_alternatives, LCEL]
---

# å¦‚ä½•é…ç½®è¿è¡Œæ—¶é“¾å†…éƒ¨

:::info å‰ææ¡ä»¶

æœ¬æŒ‡å—å‡è®¾æ‚¨ç†Ÿæ‚‰ä»¥ä¸‹æ¦‚å¿µï¼š
- [LangChain è¡¨è¾¾å¼è¯­è¨€ (LCEL)](/docs/concepts/#langchain-expression-language)
- [é“¾æ¥å¯è¿è¡Œå¯¹è±¡](/docs/how_to/sequence/)
- [ç»‘å®šè¿è¡Œæ—¶å‚æ•°](/docs/how_to/binding/)

:::

æœ‰æ—¶æ‚¨å¯èƒ½å¸Œæœ›åœ¨é“¾ä¸­å°è¯•å¤šç§ä¸åŒçš„æ–¹å¼ï¼Œç”šè‡³å‘æœ€ç»ˆç”¨æˆ·å±•ç¤ºè¿™äº›æ–¹å¼ã€‚è¿™å¯ä»¥åŒ…æ‹¬è°ƒæ•´æ¸©åº¦ç­‰å‚æ•°ï¼Œç”šè‡³å°†ä¸€ä¸ªæ¨¡å‹æ›¿æ¢ä¸ºå¦ä¸€ä¸ªæ¨¡å‹ã€‚ä¸ºäº†ä½¿è¿™ä¸€ä½“éªŒå°½å¯èƒ½ç®€å•ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸¤ç§æ–¹æ³•ã€‚

- `configurable_fields` æ–¹æ³•ã€‚æ­¤æ–¹æ³•å…è®¸æ‚¨é…ç½®å¯è¿è¡Œå¯¹è±¡çš„ç‰¹å®šå­—æ®µã€‚
  - è¿™ä¸å¯è¿è¡Œå¯¹è±¡ä¸Šçš„ [`.bind`](/docs/how_to/binding) æ–¹æ³•ç›¸å…³ï¼Œä½†å…è®¸æ‚¨åœ¨è¿è¡Œæ—¶ä¸ºé“¾ä¸­çš„ç‰¹å®šæ­¥éª¤æŒ‡å®šå‚æ•°ï¼Œè€Œä¸æ˜¯äº‹å…ˆæŒ‡å®šã€‚
- `configurable_alternatives` æ–¹æ³•ã€‚é€šè¿‡æ­¤æ–¹æ³•ï¼Œæ‚¨å¯ä»¥åˆ—å‡ºåœ¨è¿è¡Œæ—¶å¯ä»¥è®¾ç½®çš„ä»»ä½•ç‰¹å®šå¯è¿è¡Œå¯¹è±¡çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸ºé‚£äº›æŒ‡å®šçš„æ›¿ä»£æ–¹æ¡ˆã€‚

## å¯é…ç½®å­—æ®µ

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥æ¼”ç¤ºå¦‚ä½•åœ¨è¿è¡Œæ—¶é…ç½®èŠå¤©æ¨¡å‹å­—æ®µï¼Œä¾‹å¦‚æ¸©åº¦ï¼š


```python
%pip install --upgrade --quiet langchain langchain-openai

import os
from getpass import getpass

os.environ["OPENAI_API_KEY"] = getpass()
```


```python
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI

model = ChatOpenAI(temperature=0).configurable_fields(
    temperature=ConfigurableField(
        id="llm_temperature",
        name="LLM æ¸©åº¦",
        description="LLM çš„æ¸©åº¦",
    )
)

model.invoke("pick a random number")
```



```output
AIMessage(content='17', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba26a0da-0a69-4533-ab7f-21178a73d303-0')
```


åœ¨ä¸Šé¢ï¼Œæˆ‘ä»¬å°† `temperature` å®šä¹‰ä¸ºä¸€ä¸ª [`ConfigurableField`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField)ï¼Œå¯ä»¥åœ¨è¿è¡Œæ—¶è¿›è¡Œè®¾ç½®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ [`with_config`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_config) æ–¹æ³•ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š


```python
model.with_config(configurable={"llm_temperature": 0.9}).invoke("pick a random number")
```



```output
AIMessage(content='12', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba8422ad-be77-4cb1-ac45-ad0aae74e3d9-0')
```


è¯·æ³¨æ„ï¼Œå­—å…¸ä¸­ä¼ é€’çš„ `llm_temperature` æ¡ç›®çš„é”®ä¸ `ConfigurableField` çš„ `id` ç›¸åŒã€‚

æˆ‘ä»¬è¿˜å¯ä»¥è¿™æ ·åšï¼Œåªå½±å“é“¾ä¸­æŸä¸€æ­¥ï¼š


```python
prompt = PromptTemplate.from_template("Pick a random number above {x}")
chain = prompt | model

chain.invoke({"x": 0})
```



```output
AIMessage(content='27', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ecd4cadd-1b72-4f92-b9a0-15e08091f537-0')
```



```python
chain.with_config(configurable={"llm_temperature": 0.9}).invoke({"x": 0})
```



```output
AIMessage(content='35', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-a916602b-3460-46d3-a4a8-7c926ec747c0-0')
```

### ä½¿ç”¨ HubRunnables

è¿™å¯¹äºåˆ‡æ¢æç¤ºéå¸¸æœ‰ç”¨


```python
from langchain.runnables.hub import HubRunnable

prompt = HubRunnable("rlm/rag-prompt").configurable_fields(
    owner_repo_commit=ConfigurableField(
        id="hub_commit",
        name="Hub Commit",
        description="è¦æ‹‰å–çš„ Hub æäº¤",
    )
)

prompt.invoke({"question": "foo", "context": "bar"})
```



```output
ChatPromptValue(messages=[HumanMessage(content="ä½ æ˜¯ä¸€ä¸ªç”¨äºé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µæ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯å¹¶ä¿æŒç­”æ¡ˆç®€æ´ã€‚\né—®é¢˜: foo \nä¸Šä¸‹æ–‡: bar \nç­”æ¡ˆ:")])
```



```python
prompt.with_config(configurable={"hub_commit": "rlm/rag-prompt-llama"}).invoke(
    {"question": "foo", "context": "bar"}
)
```



```output
ChatPromptValue(messages=[HumanMessage(content="[INST]<<SYS>> ä½ æ˜¯ä¸€ä¸ªç”¨äºé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µæ¥å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯å¹¶ä¿æŒç­”æ¡ˆç®€æ´ã€‚<</SYS>> \né—®é¢˜: foo \nä¸Šä¸‹æ–‡: bar \nç­”æ¡ˆ: [/INST]")])
```

## å¯é…ç½®çš„æ›¿ä»£æ–¹æ¡ˆ



`configurable_alternatives()` æ–¹æ³•å…è®¸æˆ‘ä»¬åœ¨é“¾ä¸­ç”¨æ›¿ä»£æ­¥éª¤è¿›è¡Œæ›¿æ¢ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªèŠå¤©æ¨¡å‹æ›¿æ¢ä¸ºå¦ä¸€ä¸ªï¼š


```python
%pip install --upgrade --quiet langchain-anthropic

import os
from getpass import getpass

os.environ["ANTHROPIC_API_KEY"] = getpass()
```
```output
[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.
You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m
[0mNote: you may need to restart the kernel to use updated packages.
```

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI

llm = ChatAnthropic(
    model="claude-3-haiku-20240307", temperature=0
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="llm"),
    # This sets a default_key.
    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used
    default_key="anthropic",
    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`
    openai=ChatOpenAI(),
    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model="gpt-4")`
    gpt4=ChatOpenAI(model="gpt-4"),
    # You can add more configuration options here
)
prompt = PromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm

# By default it will call Anthropic
chain.invoke({"topic": "bears"})
```



```output
AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!\n\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!", response_metadata={'id': 'msg_018edUHh5fUbWdiimhrC3dZD', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-775bc58c-28d7-4e6b-a268-48fa6661f02f-0')
```



```python
# We can use `.with_config(configurable={"llm": "openai"})` to specify an llm to use
chain.with_config(configurable={"llm": "openai"}).invoke({"topic": "bears"})
```



```output
AIMessage(content="Why don't bears like fast food?\n\nBecause they can't catch it!", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-7bdaa992-19c9-4f0d-9a0c-1f326bc992d4-0')
```



```python
# If we use the `default_key` then it uses the default
chain.with_config(configurable={"llm": "anthropic"}).invoke({"topic": "bears"})
```



```output
AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!\n\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!", response_metadata={'id': 'msg_01BZvbmnEPGBtcxRWETCHkct', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-59b6ee44-a1cd-41b8-a026-28ee67cdd718-0')
```

### ä½¿ç”¨æç¤º

æˆ‘ä»¬å¯ä»¥åšç±»ä¼¼çš„äº‹æƒ…ï¼Œä½†åœ¨æç¤ºä¹‹é—´äº¤æ›¿

```python
llm = ChatAnthropic(model="claude-3-haiku-20240307", temperature=0)
prompt = PromptTemplate.from_template(
    "å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äº {topic} çš„ç¬‘è¯"
).configurable_alternatives(
    # è¿™ç»™è¿™ä¸ªå­—æ®µä¸€ä¸ª id
    # å½“é…ç½®æœ€ç»ˆå¯è¿è¡Œé¡¹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ª id æ¥é…ç½®è¿™ä¸ªå­—æ®µ
    ConfigurableField(id="prompt"),
    # è¿™è®¾ç½®äº†ä¸€ä¸ª default_keyã€‚
    # å¦‚æœæˆ‘ä»¬æŒ‡å®šè¿™ä¸ªé”®ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„ LLMï¼ˆä¸Šè¿°åˆå§‹åŒ–çš„ ChatAnthropicï¼‰
    default_key="joke",
    # è¿™æ·»åŠ äº†ä¸€ä¸ªæ–°é€‰é¡¹ï¼Œåç§°ä¸º `poem`
    poem=PromptTemplate.from_template("å†™ä¸€é¦–å…³äº {topic} çš„çŸ­è¯—"),
    # ä½ å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé…ç½®é€‰é¡¹
)
chain = prompt | llm

# é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒå°†å†™ä¸€ä¸ªç¬‘è¯
chain.invoke({"topic": "bears"})
```

```output
AIMessage(content="è¿™æ˜¯ä¸€ä¸ªå…³äºç†Šçš„ç¬‘è¯ï¼š\n\nä¸ºä»€ä¹ˆç†Šä¸ç©¿è¢œå­ï¼Ÿ \nå› ä¸ºå®ƒä»¬æœ‰ç†Šè„šï¼", response_metadata={'id': 'msg_01DtM1cssjNFZYgeS3gMZ49H', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 28}}, id='run-8199af7d-ea31-443d-b064-483693f2e0a1-0')
```

```python
# æˆ‘ä»¬å¯ä»¥é…ç½®å®ƒå†™ä¸€é¦–è¯—
chain.with_config(configurable={"prompt": "poem"}).invoke({"topic": "bears"})
```

```output
AIMessage(content="è¿™æ˜¯å…³äºç†Šçš„çŸ­è¯—ï¼š\n\né›„ä¼Ÿçš„ç†Šï¼Œå¼ºå£®è€ŒçœŸå®ï¼Œ\nåœ¨æ£®æ—ä¸­æ¸¸è¡ï¼Œè‡ªç”±è€Œç‹‚é‡ã€‚\nå¼ºå¤§çš„çˆªå­ï¼ŒæŸ”è½¯çš„æ£•è‰²æ¯›å‘ï¼Œ\nå¨ä¸¥çš„å°Šé‡ï¼Œè‡ªç„¶çš„ç‹å† ã€‚\n\nè§…é£Ÿæµ†æœï¼Œæ•é±¼æºªæµï¼Œ\nä¿æŠ¤å¹¼å´½ï¼Œå‡¶çŒ›è€Œæ•é”ã€‚\nå¼ºå¤§çš„ç†Šï¼Œä»¤äººç©ç›®çš„æ™¯è±¡ï¼Œ\nè’é‡çš„å®ˆæŠ¤è€…ï¼Œæœªæ›¾è¯‰è¯´ã€‚\n\nåœ¨é‡å¤–å®ƒä»¬ç»Ÿæ²»è‡³é«˜æ— ä¸Šï¼Œ\nä½“ç°è‡ªç„¶çš„å®ä¼Ÿä¸»é¢˜ã€‚\nç†Šï¼ŒåŠ›é‡ä¸ä¼˜é›…çš„è±¡å¾ï¼Œ\nå¸å¼•ç€æ‰€æœ‰çœ‹åˆ°å®ƒä»¬é¢å­”çš„äººã€‚", response_metadata={'id': 'msg_01Wck3qPxrjURtutvtodaJFn', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 134}}, id='run-69414a1e-51d7-4bec-a307-b34b7d61025e-0')
```

### ä½¿ç”¨æç¤ºå’ŒLLMs

æˆ‘ä»¬ä¹Ÿå¯ä»¥é…ç½®å¤šä¸ªå†…å®¹ï¼
è¿™æ˜¯ä¸€ä¸ªåŒæ—¶ä½¿ç”¨æç¤ºå’ŒLLMsçš„ç¤ºä¾‹ã€‚

```python
llm = ChatAnthropic(
    model="claude-3-haiku-20240307", temperature=0
).configurable_alternatives(
    # è¿™ç»™è¿™ä¸ªå­—æ®µä¸€ä¸ªid
    # åœ¨é…ç½®æœ€ç»ˆå¯è¿è¡Œçš„å†…å®¹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªidæ¥é…ç½®è¿™ä¸ªå­—æ®µ
    ConfigurableField(id="llm"),
    # è¿™è®¾ç½®ä¸€ä¸ªdefault_keyã€‚
    # å¦‚æœæˆ‘ä»¬æŒ‡å®šè¿™ä¸ªkeyï¼Œå°†ä½¿ç”¨é»˜è®¤çš„LLMï¼ˆä¸Šè¿°åˆå§‹åŒ–çš„ChatAnthropicï¼‰
    default_key="anthropic",
    # è¿™æ·»åŠ ä¸€ä¸ªæ–°é€‰é¡¹ï¼Œåç§°ä¸º`openai`ï¼Œç­‰äº`ChatOpenAI()`
    openai=ChatOpenAI(),
    # è¿™æ·»åŠ ä¸€ä¸ªæ–°é€‰é¡¹ï¼Œåç§°ä¸º`gpt4`ï¼Œç­‰äº`ChatOpenAI(model="gpt-4")`
    gpt4=ChatOpenAI(model="gpt-4"),
    # ä½ å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé…ç½®é€‰é¡¹
)
prompt = PromptTemplate.from_template(
    "å‘Šè¯‰æˆ‘ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯"
).configurable_alternatives(
    # è¿™ç»™è¿™ä¸ªå­—æ®µä¸€ä¸ªid
    # åœ¨é…ç½®æœ€ç»ˆå¯è¿è¡Œçš„å†…å®¹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªidæ¥é…ç½®è¿™ä¸ªå­—æ®µ
    ConfigurableField(id="prompt"),
    # è¿™è®¾ç½®ä¸€ä¸ªdefault_keyã€‚
    # å¦‚æœæˆ‘ä»¬æŒ‡å®šè¿™ä¸ªkeyï¼Œå°†ä½¿ç”¨é»˜è®¤çš„LLMï¼ˆä¸Šè¿°åˆå§‹åŒ–çš„ChatAnthropicï¼‰
    default_key="joke",
    # è¿™æ·»åŠ ä¸€ä¸ªæ–°é€‰é¡¹ï¼Œåç§°ä¸º`poem`
    poem=PromptTemplate.from_template("å†™ä¸€é¦–å…³äº{topic}çš„çŸ­è¯—"),
    # ä½ å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé…ç½®é€‰é¡¹
)
chain = prompt | llm

# æˆ‘ä»¬å¯ä»¥é…ç½®å®ƒå†™ä¸€é¦–ç”¨OpenAIçš„è¯—
chain.with_config(configurable={"prompt": "poem", "llm": "openai"}).invoke(
    {"topic": "bears"}
)
```



```output
AIMessage(content="åœ¨å¹¿é˜”è€Œæ·±é‚ƒçš„æ£®æ—ä¸­ï¼Œ\nç†Šä»¥ä¼˜é›…å’Œéª„å‚²æ¼«æ¸¸ã€‚\næ¯›å‘å¦‚å¤œè‰²èˆ¬é»‘æš—ï¼Œ\nå®ƒä»¬ä»¥å…¨éƒ¨çš„åŠ›é‡ç»Ÿæ²»è¿™ç‰‡åœŸåœ°ã€‚\n\nåœ¨å†¬å¤©çš„å¯’å†·ä¸­ï¼Œå®ƒä»¬å†¬çœ ï¼Œ\næ˜¥å¤©å®ƒä»¬è‹é†’ï¼Œé¥¥é¥¿è€Œä¼Ÿå¤§ã€‚\né”‹åˆ©çš„çˆªå­å’Œæ•é”çš„çœ¼ç›ï¼Œ\nå®ƒä»¬å¯»æ‰¾é£Ÿç‰©ï¼Œå‡¶çŒ›è€Œç˜¦å‰Šã€‚\n\nä½†åœ¨å®ƒä»¬åšç¡¬çš„å¤–è¡¨ä¸‹ï¼Œ\nè—ç€ä¸€é¢—æ¸©æš–è€Œé«˜è´µçš„å¿ƒã€‚\nå®ƒä»¬ç”¨å°½å…¨åŠ›çˆ±æŠ¤å¹¼å´½ï¼Œ\nåœ¨ç™½å¤©å’Œé»‘å¤œä¸­ä¿æŠ¤å®ƒä»¬ã€‚\n\næ‰€ä»¥è®©æˆ‘ä»¬æ¬£èµè¿™äº›é›„ä¼Ÿçš„ç”Ÿç‰©ï¼Œ\nå¯¹å®ƒä»¬çš„åŠ›é‡å’Œç‰¹å¾æ„Ÿåˆ°æ•¬ç•ã€‚\nå› ä¸ºåœ¨é‡å¤–ï¼Œå®ƒä»¬æ˜¯è‡³é«˜æ— ä¸Šçš„ï¼Œ\nå¼ºå¤§çš„ç†Šï¼Œæ°¸æ’çš„æ¢¦æƒ³ã€‚", response_metadata={'token_usage': {'completion_tokens': 133, 'prompt_tokens': 13, 'total_tokens': 146}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-5eec0b96-d580-49fd-ac4e-e32a0803b49b-0')
```



```python
# å¦‚æœæˆ‘ä»¬åªæƒ³é…ç½®ä¸€ä¸ªï¼Œä¹Ÿå¯ä»¥
chain.with_config(configurable={"llm": "openai"}).invoke({"topic": "bears"})
```



```output
AIMessage(content="ä¸ºä»€ä¹ˆç†Šä¸ç©¿é‹å­ï¼Ÿ\n\nå› ä¸ºå®ƒä»¬æœ‰ç†ŠæŒï¼", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-c1b14c9c-4988-49b8-9363-15bfd479973a-0')
```

### ä¿å­˜é…ç½®

æˆ‘ä»¬è¿˜å¯ä»¥è½»æ¾åœ°å°†é…ç½®å¥½çš„é“¾ä¿å­˜ä¸ºå®ƒä»¬è‡ªå·±çš„å¯¹è±¡


```python
openai_joke = chain.with_config(configurable={"llm": "openai"})

openai_joke.invoke({"topic": "bears"})
```



```output
AIMessage(content="Why did the bear break up with his girlfriend? \nBecause he couldn't bear the relationship anymore!", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-391ebd55-9137-458b-9a11-97acaff6a892-0')
```

## ä¸‹ä¸€æ­¥

æ‚¨ç°åœ¨çŸ¥é“å¦‚ä½•åœ¨è¿è¡Œæ—¶é…ç½®é“¾çš„å†…éƒ¨æ­¥éª¤ã€‚

è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æœ¬èŠ‚ä¸­å…³äºå¯è¿è¡Œå¯¹è±¡çš„å…¶ä»–æ“ä½œæŒ‡å—ï¼ŒåŒ…æ‹¬ï¼š

- ä½¿ç”¨ [.bind()](/docs/how_to/binding) ä½œä¸ºè®¾ç½®å¯è¿è¡Œå¯¹è±¡è¿è¡Œæ—¶å‚æ•°çš„æ›´ç®€å•æ–¹æ³•