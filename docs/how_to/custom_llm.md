---
custom_edit_url: https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/custom_llm.ipynb
---

# å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ LLM ç±»

æœ¬ç¬”è®°æœ¬ä»‹ç»äº†å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ LLM åŒ…è£…å™¨ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥ä½¿ç”¨è‡ªå·±çš„ LLM æˆ–ä¸ LangChain æ”¯æŒçš„åŒ…è£…å™¨ä¸åŒçš„åŒ…è£…å™¨ã€‚

ä½¿ç”¨æ ‡å‡† `LLM` æ¥å£åŒ…è£…æ‚¨çš„ LLMï¼Œå¯ä»¥è®©æ‚¨åœ¨ç°æœ‰çš„ LangChain ç¨‹åºä¸­ä»¥æœ€å°çš„ä»£ç ä¿®æ”¹ä½¿ç”¨æ‚¨çš„ LLMï¼

ä½œä¸ºé¢å¤–çš„å¥½å¤„ï¼Œæ‚¨çš„ LLM å°†è‡ªåŠ¨æˆä¸º LangChain `Runnable`ï¼Œå¹¶å°†äº«å—å¼€ç®±å³ç”¨çš„ä¸€äº›ä¼˜åŒ–ã€å¼‚æ­¥æ”¯æŒã€`astream_events` API ç­‰ã€‚

## å®ç°

è‡ªå®šä¹‰ LLM éœ€è¦å®ç°çš„ä¸¤ä¸ªå¿…éœ€å†…å®¹ï¼š

| æ–¹æ³•          | æè¿°                                                                  |
|---------------|-----------------------------------------------------------------------|
| `_call`       | æ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²å’Œä¸€äº›å¯é€‰çš„åœæ­¢è¯ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚ç”± `invoke` ä½¿ç”¨ã€‚ |
| `_llm_type`   | è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²çš„å±æ€§ï¼Œä»…ç”¨äºæ—¥å¿—è®°å½•ç›®çš„ã€‚                           |

å¯é€‰å®ç°ï¼š

| æ–¹æ³•                  | æè¿°                                                                                                  |
|----------------------|-------------------------------------------------------------------------------------------------------|
| `_identifying_params` | ç”¨äºå¸®åŠ©è¯†åˆ«æ¨¡å‹å¹¶æ‰“å° LLMï¼›åº”è¿”å›ä¸€ä¸ªå­—å…¸ã€‚è¿™æ˜¯ä¸€ä¸ª **@property**ã€‚                                 |
| `_acall`              | æä¾› `_call` çš„å¼‚æ­¥æœ¬åœ°å®ç°ï¼Œç”± `ainvoke` ä½¿ç”¨ã€‚                                                     |
| `_stream`             | æ–¹æ³•é€ä¸ªè¾“å‡ºæµå¼ç”Ÿæˆçš„ä»¤ç‰Œã€‚                                                                          |
| `_astream`            | æä¾› `_stream` çš„å¼‚æ­¥æœ¬åœ°å®ç°ï¼›åœ¨è¾ƒæ–°çš„ LangChain ç‰ˆæœ¬ä¸­ï¼Œé»˜è®¤ä¸º `_stream`ã€‚                        |

è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„è‡ªå®šä¹‰ LLMï¼Œä»…è¿”å›è¾“å…¥çš„å‰ n ä¸ªå­—ç¬¦ã€‚

```python
from typing import Any, Dict, Iterator, List, Mapping, Optional

from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk


class CustomLLM(LLM):
    """ä¸€ä¸ªè‡ªå®šä¹‰èŠå¤©æ¨¡å‹ï¼Œå›æ˜¾è¾“å…¥çš„å‰ `n` ä¸ªå­—ç¬¦ã€‚

    åœ¨å‘ LangChain æäº¤å®ç°æ—¶ï¼Œè¯·ä»”ç»†è®°å½•æ¨¡å‹ï¼ŒåŒ…æ‹¬åˆå§‹åŒ–å‚æ•°ï¼Œ
    åŒ…å«å¦‚ä½•åˆå§‹åŒ–æ¨¡å‹çš„ç¤ºä¾‹ï¼Œå¹¶åŒ…å«ä»»ä½•ç›¸å…³çš„
    åº•å±‚æ¨¡å‹æ–‡æ¡£æˆ– API çš„é“¾æ¥ã€‚

    ç¤ºä¾‹ï¼š

        .. code-block:: python

            model = CustomChatModel(n=2)
            result = model.invoke([HumanMessage(content="hello")])
            result = model.batch([[HumanMessage(content="hello")],
                                 [HumanMessage(content="world")]])
    """

    n: int
    """ä»æç¤ºçš„æœ€åä¸€æ¡æ¶ˆæ¯ä¸­å›æ˜¾çš„å­—ç¬¦æ•°ã€‚"""

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """åœ¨ç»™å®šè¾“å…¥ä¸Šè¿è¡Œ LLMã€‚

        é‡å†™æ­¤æ–¹æ³•ä»¥å®ç° LLM é€»è¾‘ã€‚

        å‚æ•°ï¼š
            prompt: è¦ç”Ÿæˆçš„æç¤ºã€‚
            stop: ç”Ÿæˆæ—¶ä½¿ç”¨çš„åœæ­¢è¯ã€‚æ¨¡å‹è¾“å‡ºåœ¨ä»»ä½•åœæ­¢å­å­—ç¬¦ä¸²çš„ç¬¬ä¸€æ¬¡å‡ºç°å¤„è¢«æˆªæ–­ã€‚
                å¦‚æœä¸æ”¯æŒåœæ­¢ä»¤ç‰Œï¼Œè¯·è€ƒè™‘å¼•å‘ NotImplementedErrorã€‚
            run_manager: è¿è¡Œçš„å›è°ƒç®¡ç†å™¨ã€‚
            **kwargs: ä»»æ„å…¶ä»–å…³é”®å­—å‚æ•°ã€‚è¿™äº›é€šå¸¸ä¼ é€’ç»™æ¨¡å‹æä¾›è€… API è°ƒç”¨ã€‚

        è¿”å›ï¼š
            æ¨¡å‹è¾“å‡ºä½œä¸ºå­—ç¬¦ä¸²ã€‚å®é™…çš„å®Œæˆä¸åº”åŒ…å«æç¤ºã€‚
        """
        if stop is not None:
            raise ValueError("ä¸å…è®¸ä½¿ç”¨ stop kwargsã€‚")
        return prompt[: self.n]

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        """åœ¨ç»™å®šæç¤ºä¸Šæµå¼ä¼ è¾“ LLMã€‚

        æ”¯æŒæµå¼ä¼ è¾“çš„å­ç±»åº”é‡å†™æ­¤æ–¹æ³•ã€‚

        å¦‚æœæœªå®ç°ï¼Œå¯¹æµçš„è°ƒç”¨çš„é»˜è®¤è¡Œä¸ºå°†å›é€€åˆ°æ¨¡å‹çš„éæµå¼ç‰ˆæœ¬ï¼Œå¹¶å°†
        è¾“å‡ºä½œä¸ºä¸€ä¸ªå•ç‹¬çš„å—è¿”å›ã€‚

        å‚æ•°ï¼š
            prompt: è¦ç”Ÿæˆçš„æç¤ºã€‚
            stop: ç”Ÿæˆæ—¶ä½¿ç”¨çš„åœæ­¢è¯ã€‚æ¨¡å‹è¾“å‡ºåœ¨è¿™äº›å­å­—ç¬¦ä¸²çš„ç¬¬ä¸€æ¬¡å‡ºç°å¤„è¢«æˆªæ–­ã€‚
            run_manager: è¿è¡Œçš„å›è°ƒç®¡ç†å™¨ã€‚
            **kwargs: ä»»æ„å…¶ä»–å…³é”®å­—å‚æ•°ã€‚è¿™äº›é€šå¸¸ä¼ é€’ç»™æ¨¡å‹æä¾›è€… API è°ƒç”¨ã€‚

        è¿”å›ï¼š
            GenerationChunks çš„è¿­ä»£å™¨ã€‚
        """
        for char in prompt[: self.n]:
            chunk = GenerationChunk(text=char)
            if run_manager:
                run_manager.on_llm_new_token(chunk.text, chunk=chunk)

            yield chunk

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è¿”å›ä¸€ä¸ªè¯†åˆ«å‚æ•°çš„å­—å…¸ã€‚"""
        return {
            # æ¨¡å‹åç§°å…è®¸ç”¨æˆ·åœ¨ LLM ç›‘æ§åº”ç”¨ä¸­æŒ‡å®šè‡ªå®šä¹‰ä»¤ç‰Œè®¡æ•°
            # è§„åˆ™ï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangSmith ä¸­ï¼Œç”¨æˆ·å¯ä»¥ä¸ºå…¶æ¨¡å‹æä¾›æ¯ä¸ªä»¤ç‰Œå®šä»·å¹¶ç›‘æ§
            # ç»™å®š LLM çš„æˆæœ¬ã€‚ï¼‰
            "model_name": "CustomChatModel",
        }

    @property
    def _llm_type(self) -> str:
        """è·å–æ­¤èŠå¤©æ¨¡å‹ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹çš„ç±»å‹ã€‚ä»…ç”¨äºæ—¥å¿—è®°å½•ç›®çš„ã€‚"""
        return "custom"
```

### è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ ğŸ§ª

è¿™ä¸ª LLM å°†å®ç° LangChain çš„æ ‡å‡† `Runnable` æ¥å£ï¼Œè®¸å¤š LangChain æŠ½è±¡éƒ½æ”¯æŒå®ƒï¼


```python
llm = CustomLLM(n=5)
print(llm)
```
```output
[1mCustomLLM[0m
Params: {'model_name': 'CustomChatModel'}
```

```python
llm.invoke("This is a foobar thing")
```



```output
'This '
```



```python
await llm.ainvoke("world")
```



```output
'world'
```



```python
llm.batch(["woof woof woof", "meow meow meow"])
```



```output
['woof ', 'meow ']
```



```python
await llm.abatch(["woof woof woof", "meow meow meow"])
```



```output
['woof ', 'meow ']
```



```python
async for token in llm.astream("hello"):
    print(token, end="|", flush=True)
```
```output
h|e|l|l|o|
```
è®©æˆ‘ä»¬ç¡®è®¤å®ƒä¸å…¶ä»– `LangChain` API çš„è‰¯å¥½é›†æˆã€‚


```python
from langchain_core.prompts import ChatPromptTemplate
```


```python
prompt = ChatPromptTemplate.from_messages(
    [("system", "you are a bot"), ("human", "{input}")]
)
```


```python
llm = CustomLLM(n=7)
chain = prompt | llm
```


```python
idx = 0
async for event in chain.astream_events({"input": "hello there!"}, version="v1"):
    print(event)
    idx += 1
    if idx > 7:
        # Truncate
        break
```
```output
{'event': 'on_chain_start', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}
{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}
{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}, 'output': ChatPromptValue(messages=[SystemMessage(content='you are a bot'), HumanMessage(content='hello there!')])}}
{'event': 'on_llm_start', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'prompts': ['System: you are a bot\nHuman: hello there!']}}}
{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'S'}}
{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'S'}}
{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'y'}}
{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'y'}}
```

## è´¡çŒ®

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰èŠå¤©æ¨¡å‹é›†æˆçš„è´¡çŒ®ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ£€æŸ¥æ¸…å•ï¼Œä»¥å¸®åŠ©ç¡®ä¿æ‚¨çš„è´¡çŒ®è¢«æ·»åŠ åˆ° LangChainï¼š

æ–‡æ¡£ï¼š

* æ¨¡å‹åŒ…å«æ‰€æœ‰åˆå§‹åŒ–å‚æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå› ä¸ºè¿™äº›å°†åœ¨ [APIReference](https://api.python.langchain.com/en/stable/langchain_api_reference.html) ä¸­æ˜¾ç¤ºã€‚
* å¦‚æœæ¨¡å‹ç”±æœåŠ¡æä¾›æ”¯æŒï¼Œæ¨¡å‹çš„ç±»æ–‡æ¡£å­—ç¬¦ä¸²ä¸­åº”åŒ…å«æŒ‡å‘æ¨¡å‹ API çš„é“¾æ¥ã€‚

æµ‹è¯•ï¼š

* [ ] ä¸ºé‡å†™çš„æ–¹æ³•æ·»åŠ å•å…ƒæˆ–é›†æˆæµ‹è¯•ã€‚å¦‚æœæ‚¨é‡å†™äº†ç›¸åº”çš„ä»£ç ï¼Œè¯·éªŒè¯ `invoke`ã€`ainvoke`ã€`batch`ã€`stream` æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

æµå¼å¤„ç†ï¼ˆå¦‚æœæ‚¨æ­£åœ¨å®ç°ï¼‰ï¼š

* [ ] ç¡®ä¿è°ƒç”¨ `on_llm_new_token` å›è°ƒ
* [ ] `on_llm_new_token` åœ¨ç”Ÿæˆå—ä¹‹å‰è¢«è°ƒç”¨

åœæ­¢ä»¤ç‰Œè¡Œä¸ºï¼š

* [ ] åº”å°Šé‡åœæ­¢ä»¤ç‰Œ
* [ ] åœæ­¢ä»¤ç‰Œåº”ä½œä¸ºå“åº”çš„ä¸€éƒ¨åˆ†åŒ…å«åœ¨å†…

ç§˜å¯† API å¯†é’¥ï¼š

* [ ] å¦‚æœæ‚¨çš„æ¨¡å‹è¿æ¥åˆ° APIï¼Œå®ƒå¯èƒ½ä¼šåœ¨åˆå§‹åŒ–æ—¶æ¥å— API å¯†é’¥ã€‚ä½¿ç”¨ Pydantic çš„ `SecretStr` ç±»å‹æ¥å¤„ç†ç§˜å¯†ï¼Œä»¥ä¾¿åœ¨æ‰“å°æ¨¡å‹æ—¶ä¸ä¼šæ„å¤–æ‰“å°å‡ºæ¥ã€‚