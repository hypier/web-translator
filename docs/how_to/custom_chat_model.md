---
custom_edit_url: https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/custom_chat_model.ipynb
---

# å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰èŠå¤©æ¨¡å‹ç±»

:::info å…ˆå†³æ¡ä»¶

æœ¬æŒ‡å—å‡è®¾æ‚¨å¯¹ä»¥ä¸‹æ¦‚å¿µæœ‰ä¸€å®šäº†è§£ï¼š
- [èŠå¤©æ¨¡å‹](/docs/concepts/#chat-models)

:::

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ LangChain æŠ½è±¡åˆ›å»ºè‡ªå®šä¹‰èŠå¤©æ¨¡å‹ã€‚

ä½¿ç”¨æ ‡å‡† [`BaseChatModel`](https://api.python.langchain.com/en/latest/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) æ¥å£åŒ…è£…æ‚¨çš„ LLMï¼Œå¯ä»¥è®©æ‚¨åœ¨ç°æœ‰çš„ LangChain ç¨‹åºä¸­ä»¥æœ€å°çš„ä»£ç ä¿®æ”¹ä½¿ç”¨æ‚¨çš„ LLMï¼

ä½œä¸ºé¢å¤–å¥½å¤„ï¼Œæ‚¨çš„ LLM å°†è‡ªåŠ¨æˆä¸º LangChain `Runnable`ï¼Œå¹¶å°†å—ç›Šäºä¸€äº›å¼€ç®±å³ç”¨çš„ä¼˜åŒ–ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡çº¿ç¨‹æ± æ‰¹å¤„ç†ï¼‰ã€å¼‚æ­¥æ”¯æŒã€`astream_events` API ç­‰ã€‚

## è¾“å…¥å’Œè¾“å‡º

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¨è®ºä¸€ä¸‹ **æ¶ˆæ¯**ï¼Œå®ƒä»¬æ˜¯èŠå¤©æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºã€‚

### æ¶ˆæ¯

èŠå¤©æ¨¡å‹å°†æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ¶ˆæ¯ä½œä¸ºè¾“å‡ºã€‚

LangChain æœ‰å‡ ç§ [å†…ç½®æ¶ˆæ¯ç±»å‹](/docs/concepts/#message-types)ï¼š

| æ¶ˆæ¯ç±»å‹              | æè¿°                                                                                          |
|-----------------------|-------------------------------------------------------------------------------------------------|
| `SystemMessage`       | ç”¨äºåˆå§‹åŒ– AI è¡Œä¸ºï¼Œé€šå¸¸ä½œä¸ºè¾“å…¥æ¶ˆæ¯åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªä¼ å…¥ã€‚                                         |
| `HumanMessage`        | ä»£è¡¨ä¸èŠå¤©æ¨¡å‹äº’åŠ¨çš„äººçš„æ¶ˆæ¯ã€‚                                                                |
| `AIMessage`           | ä»£è¡¨æ¥è‡ªèŠå¤©æ¨¡å‹çš„æ¶ˆæ¯ã€‚è¿™å¯ä»¥æ˜¯æ–‡æœ¬æˆ–è¯·æ±‚è°ƒç”¨å·¥å…·ã€‚                                          |
| `FunctionMessage` / `ToolMessage` | ç”¨äºå°†å·¥å…·è°ƒç”¨çš„ç»“æœä¼ å›æ¨¡å‹çš„æ¶ˆæ¯ã€‚                                      |
| `AIMessageChunk` / `HumanMessageChunk` / ... | æ¯ç§æ¶ˆæ¯ç±»å‹çš„å—å˜ä½“ã€‚ |


::: {.callout-note}
`ToolMessage` å’Œ `FunctionMessage` ç´§å¯†éµå¾ª OpenAI çš„ `function` å’Œ `tool` è§’è‰²ã€‚

è¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œéšç€æ›´å¤šæ¨¡å‹æ·»åŠ åŠŸèƒ½è°ƒç”¨èƒ½åŠ›ï¼Œé¢„è®¡è¯¥æ¶æ„å°†ä¼šæœ‰æ–°å¢å†…å®¹ã€‚
:::


```python
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    FunctionMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
```

### æµå¼å˜ä½“

æ‰€æœ‰èŠå¤©æ¶ˆæ¯éƒ½æœ‰ä¸€ä¸ªæµå¼å˜ä½“ï¼Œå…¶åç§°ä¸­åŒ…å« `Chunk`ã€‚

```python
from langchain_core.messages import (
    AIMessageChunk,
    FunctionMessageChunk,
    HumanMessageChunk,
    SystemMessageChunk,
    ToolMessageChunk,
)
```

è¿™äº›å—åœ¨ä»èŠå¤©æ¨¡å‹æµå¼è¾“å‡ºæ—¶ä½¿ç”¨ï¼Œå®ƒä»¬éƒ½å®šä¹‰äº†ä¸€ä¸ªå¯åŠ å±æ€§ï¼

```python
AIMessageChunk(content="Hello") + AIMessageChunk(content=" World!")
```



```output
AIMessageChunk(content='Hello World!')
```

## åŸºç¡€èŠå¤©æ¨¡å‹

è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªèŠå¤©æ¨¡å‹ï¼Œå®ƒä¼šå›æ˜¾æç¤ºä¸­æœ€åä¸€æ¡æ¶ˆæ¯çš„å‰ `n` ä¸ªå­—ç¬¦ï¼

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ç»§æ‰¿ `BaseChatModel`ï¼Œå¹¶éœ€è¦å®ç°ä»¥ä¸‹å†…å®¹ï¼š

| æ–¹æ³•/å±æ€§                           | æè¿°                                                             | å¿…éœ€/å¯é€‰          |
|------------------------------------|-------------------------------------------------------------------|--------------------|
| `_generate`                        | ç”¨äºä»æç¤ºç”ŸæˆèŠå¤©ç»“æœ                                           | å¿…éœ€               |
| `_llm_type` (å±æ€§)                 | ç”¨äºå”¯ä¸€è¯†åˆ«æ¨¡å‹ç±»å‹ã€‚ç”¨äºæ—¥å¿—è®°å½•ã€‚                             | å¿…éœ€               |
| `_identifying_params` (å±æ€§)       | è¡¨ç¤ºæ¨¡å‹å‚æ•°åŒ–ä»¥ä¾¿è¿½è¸ªç›®çš„ã€‚                                     | å¯é€‰               |
| `_stream`                          | ç”¨äºå®ç°æµå¼ä¼ è¾“ã€‚                                               | å¯é€‰               |
| `_agenerate`                       | ç”¨äºå®ç°åŸç”Ÿå¼‚æ­¥æ–¹æ³•ã€‚                                           | å¯é€‰               |
| `_astream`                         | ç”¨äºå®ç° `_stream` çš„å¼‚æ­¥ç‰ˆæœ¬ã€‚                                  | å¯é€‰               |


:::tip
`_astream` å®ç°ä½¿ç”¨ `run_in_executor` åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­å¯åŠ¨åŒæ­¥çš„ `_stream`ï¼Œå¦‚æœå·²å®ç° `_stream`ï¼Œå¦åˆ™å›é€€ä½¿ç”¨ `_agenerate`ã€‚

å¦‚æœæ‚¨æƒ³é‡ç”¨ `_stream` å®ç°ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªæŠ€å·§ï¼Œä½†å¦‚æœæ‚¨èƒ½å¤Ÿå®ç°åŸç”Ÿå¼‚æ­¥çš„ä»£ç ï¼Œé‚£å°†æ˜¯æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºè¯¥ä»£ç çš„è¿è¡Œå¼€é”€æ›´å°ã€‚
:::

### å®ç°


```python
from typing import Any, AsyncIterator, Dict, Iterator, List, Optional

from langchain_core.callbacks import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)
from langchain_core.language_models import BaseChatModel, SimpleChatModel
from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage
from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
from langchain_core.runnables import run_in_executor


class CustomChatModelAdvanced(BaseChatModel):
    """ä¸€ä¸ªè‡ªå®šä¹‰èŠå¤©æ¨¡å‹ï¼Œå›æ˜¾è¾“å…¥çš„å‰ `n` ä¸ªå­—ç¬¦ã€‚

    åœ¨å‘ LangChain æäº¤å®ç°æ—¶ï¼Œä»”ç»†è®°å½•æ¨¡å‹ï¼ŒåŒ…æ‹¬åˆå§‹åŒ–å‚æ•°ï¼Œ
    åŒ…æ‹¬å¦‚ä½•åˆå§‹åŒ–æ¨¡å‹çš„ç¤ºä¾‹ï¼Œå¹¶åŒ…å«ä»»ä½•ç›¸å…³çš„åº•å±‚æ¨¡å‹æ–‡æ¡£æˆ– API çš„é“¾æ¥ã€‚

    ç¤ºä¾‹ï¼š

        .. code-block:: python

            model = CustomChatModel(n=2)
            result = model.invoke([HumanMessage(content="hello")])
            result = model.batch([[HumanMessage(content="hello")],
                                 [HumanMessage(content="world")]])
    """

    model_name: str
    """æ¨¡å‹çš„åç§°"""
    n: int
    """è¦å›æ˜¾çš„æç¤ºæœ€åä¸€æ¡æ¶ˆæ¯çš„å­—ç¬¦æ•°ã€‚"""

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """é‡å†™ _generate æ–¹æ³•ä»¥å®ç°èŠå¤©æ¨¡å‹é€»è¾‘ã€‚

        è¿™å¯ä»¥æ˜¯å¯¹ API çš„è°ƒç”¨ã€å¯¹æœ¬åœ°æ¨¡å‹çš„è°ƒç”¨æˆ–ä»»ä½•å…¶ä»–
        ç”Ÿæˆå¯¹è¾“å…¥æç¤ºçš„å“åº”çš„å®ç°ã€‚

        å‚æ•°ï¼š
            messages: ç”±æ¶ˆæ¯åˆ—è¡¨ç»„æˆçš„æç¤ºã€‚
            stop: æ¨¡å‹åº”åœæ­¢ç”Ÿæˆçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
                  å¦‚æœç”±äºåœæ­¢æ ‡è®°è€Œåœæ­¢ç”Ÿæˆï¼Œåˆ™åœæ­¢æ ‡è®°æœ¬èº«
                  åº”è¯¥ä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†åŒ…å«ã€‚è¿™åœ¨ç›®å‰çš„æ¨¡å‹ä¸­æ²¡æœ‰å¼ºåˆ¶æ‰§è¡Œï¼Œ
                  ä½†éµå¾ªè¿™ä¸€è‰¯å¥½å®è·µå¯ä»¥ä½¿åç»­è§£ææ¨¡å‹è¾“å‡º
                  æ›´åŠ å®¹æ˜“ï¼Œå¹¶ç†è§£ç”Ÿæˆåœæ­¢çš„åŸå› ã€‚
            run_manager: å¸¦æœ‰å›è°ƒçš„ LLM è¿è¡Œç®¡ç†å™¨ã€‚
        """
        # ç”¨å®é™…é€»è¾‘æ›¿æ¢æ­¤å¤„ï¼Œä»¥ä»æ¶ˆæ¯åˆ—è¡¨ç”Ÿæˆå“åº”ã€‚
        last_message = messages[-1]
        tokens = last_message.content[: self.n]
        message = AIMessage(
            content=tokens,
            additional_kwargs={},  # ç”¨äºæ·»åŠ é¢å¤–çš„è´Ÿè½½ï¼ˆä¾‹å¦‚ï¼Œå‡½æ•°è°ƒç”¨è¯·æ±‚ï¼‰
            response_metadata={  # ç”¨äºå“åº”å…ƒæ•°æ®
                "time_in_seconds": 3,
            },
        )
        ##

        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGenerationChunk]:
        """æµå¼è¾“å‡ºæ¨¡å‹çš„ç»“æœã€‚

        å¦‚æœæ¨¡å‹å¯ä»¥ä»¥æµå¼æ–¹å¼ç”Ÿæˆè¾“å‡ºï¼Œåˆ™åº”å®ç°æ­¤æ–¹æ³•ã€‚
        å¦‚æœæ¨¡å‹ä¸æ”¯æŒæµå¼å¤„ç†ï¼Œåˆ™ä¸åº”å®ç°å®ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæµå¼è¯·æ±‚å°†è‡ªåŠ¨
        ç”± _generate æ–¹æ³•å¤„ç†ã€‚

        å‚æ•°ï¼š
            messages: ç”±æ¶ˆæ¯åˆ—è¡¨ç»„æˆçš„æç¤ºã€‚
            stop: æ¨¡å‹åº”åœæ­¢ç”Ÿæˆçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
                  å¦‚æœç”±äºåœæ­¢æ ‡è®°è€Œåœæ­¢ç”Ÿæˆï¼Œåˆ™åœæ­¢æ ‡è®°æœ¬èº«
                  åº”è¯¥ä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†åŒ…å«ã€‚è¿™åœ¨ç›®å‰çš„æ¨¡å‹ä¸­æ²¡æœ‰å¼ºåˆ¶æ‰§è¡Œï¼Œ
                  ä½†éµå¾ªè¿™ä¸€è‰¯å¥½å®è·µå¯ä»¥ä½¿åç»­è§£ææ¨¡å‹è¾“å‡º
                  æ›´åŠ å®¹æ˜“ï¼Œå¹¶ç†è§£ç”Ÿæˆåœæ­¢çš„åŸå› ã€‚
            run_manager: å¸¦æœ‰å›è°ƒçš„ LLM è¿è¡Œç®¡ç†å™¨ã€‚
        """
        last_message = messages[-1]
        tokens = last_message.content[: self.n]

        for token in tokens:
            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))

            if run_manager:
                # è¿™æ˜¯åœ¨è¾ƒæ–°ç‰ˆæœ¬çš„ LangChain ä¸­æ˜¯å¯é€‰çš„
                # on_llm_new_token å°†è‡ªåŠ¨è¢«è°ƒç”¨
                run_manager.on_llm_new_token(token, chunk=chunk)

            yield chunk

        # è®©æˆ‘ä»¬æ·»åŠ ä¸€äº›å…¶ä»–ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œå“åº”å…ƒæ•°æ®ï¼‰
        chunk = ChatGenerationChunk(
            message=AIMessageChunk(content="", response_metadata={"time_in_sec": 3})
        )
        if run_manager:
            # è¿™æ˜¯åœ¨è¾ƒæ–°ç‰ˆæœ¬çš„ LangChain ä¸­æ˜¯å¯é€‰çš„
            # on_llm_new_token å°†è‡ªåŠ¨è¢«è°ƒç”¨
            run_manager.on_llm_new_token(token, chunk=chunk)
        yield chunk

    @property
    def _llm_type(self) -> str:
        """è·å–æ­¤èŠå¤©æ¨¡å‹ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ç±»å‹ã€‚"""
        return "echoing-chat-model-advanced"

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è¿”å›è¯†åˆ«å‚æ•°çš„å­—å…¸ã€‚

        æ­¤ä¿¡æ¯ç”± LangChain å›è°ƒç³»ç»Ÿä½¿ç”¨ï¼Œç”¨äºè·Ÿè¸ªç›®çš„ï¼Œä½¿ç›‘æ§ LLM æˆä¸ºå¯èƒ½ã€‚
        """
        return {
            # æ¨¡å‹åç§°å…è®¸ç”¨æˆ·åœ¨ LLM ç›‘æ§åº”ç”¨ç¨‹åºä¸­æŒ‡å®šè‡ªå®šä¹‰ä»¤ç‰Œè®¡æ•°
            # è§„åˆ™ï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangSmith ä¸­ï¼Œç”¨æˆ·å¯ä»¥ä¸ºå…¶æ¨¡å‹æä¾›æ¯ä¸ªä»¤ç‰Œçš„å®šä»·å¹¶ç›‘æ§
            # ç»™å®š LLM çš„æˆæœ¬ã€‚ï¼‰
            "model_name": self.model_name,
        }
```

### è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ ğŸ§ª

èŠå¤©æ¨¡å‹å°†å®ç° LangChain çš„æ ‡å‡† `Runnable` æ¥å£ï¼Œè®¸å¤š LangChain æŠ½è±¡éƒ½æ”¯æŒè¯¥æ¥å£ï¼


```python
model = CustomChatModelAdvanced(n=3, model_name="my_custom_model")

model.invoke(
    [
        HumanMessage(content="hello!"),
        AIMessage(content="Hi there human!"),
        HumanMessage(content="Meow!"),
    ]
)
```



```output
AIMessage(content='Meo', response_metadata={'time_in_seconds': 3}, id='run-ddb42bd6-4fdd-4bd2-8be5-e11b67d3ac29-0')
```



```python
model.invoke("hello")
```



```output
AIMessage(content='hel', response_metadata={'time_in_seconds': 3}, id='run-4d3cc912-44aa-454b-977b-ca02be06c12e-0')
```



```python
model.batch(["hello", "goodbye"])
```



```output
[AIMessage(content='hel', response_metadata={'time_in_seconds': 3}, id='run-9620e228-1912-4582-8aa1-176813afec49-0'),
 AIMessage(content='goo', response_metadata={'time_in_seconds': 3}, id='run-1ce8cdf8-6f75-448e-82f7-1bb4a121df93-0')]
```



```python
for chunk in model.stream("cat"):
    print(chunk.content, end="|")
```
```output
c|a|t||
```
è¯·æŸ¥çœ‹æ¨¡å‹ä¸­ `_astream` çš„å®ç°ï¼å¦‚æœæ‚¨æ²¡æœ‰å®ç°å®ƒï¼Œåˆ™ä¸ä¼šæœ‰è¾“å‡ºæµã€‚ï¼


```python
async for chunk in model.astream("cat"):
    print(chunk.content, end="|")
```
```output
c|a|t||
```
è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ astream äº‹ä»¶ APIï¼Œè¿™ä¹Ÿå°†å¸®åŠ©åŒé‡æ£€æŸ¥æ‰€æœ‰å›è°ƒæ˜¯å¦å·²å®ç°ï¼


```python
async for event in model.astream_events("cat", version="v1"):
    print(event)
```
```output
{'event': 'on_chat_model_start', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'name': 'CustomChatModelAdvanced', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}}
{'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='c', id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
{'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='a', id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
{'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='t', id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
{'event': 'on_chat_model_stream', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='', response_metadata={'time_in_sec': 3}, id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
{'event': 'on_chat_model_end', 'name': 'CustomChatModelAdvanced', 'run_id': '125a2a16-b9cd-40de-aa08-8aa9180b07d0', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', response_metadata={'time_in_sec': 3}, id='run-125a2a16-b9cd-40de-aa08-8aa9180b07d0')}}
``````output
/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: æ­¤ API å¤„äºæµ‹è¯•é˜¶æ®µï¼Œæœªæ¥å¯èƒ½ä¼šæ›´æ”¹ã€‚
  warn_beta(
```

## è´¡çŒ®

æˆ‘ä»¬éå¸¸æ„Ÿè°¢æ‰€æœ‰èŠå¤©æ¨¡å‹é›†æˆçš„è´¡çŒ®ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ¸…å•ï¼Œä»¥å¸®åŠ©ç¡®ä¿æ‚¨çš„è´¡çŒ®è¢«æ·»åŠ åˆ° LangChain ä¸­ï¼š

æ–‡æ¡£ï¼š

* æ¨¡å‹åŒ…å«æ‰€æœ‰åˆå§‹åŒ–å‚æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå› ä¸ºè¿™äº›å°†åœ¨ [APIReference](https://api.python.langchain.com/en/stable/langchain_api_reference.html) ä¸­æ˜¾ç¤ºã€‚
* å¦‚æœæ¨¡å‹ç”±æœåŠ¡æä¾›æ”¯æŒï¼Œåˆ™æ¨¡å‹çš„ç±»æ–‡æ¡£å­—ç¬¦ä¸²åŒ…å«æŒ‡å‘æ¨¡å‹ API çš„é“¾æ¥ã€‚

æµ‹è¯•ï¼š

* [ ] ä¸ºé‡å†™çš„æ–¹æ³•æ·»åŠ å•å…ƒæµ‹è¯•æˆ–é›†æˆæµ‹è¯•ã€‚å¦‚æœæ‚¨é‡å†™äº†ç›¸åº”çš„ä»£ç ï¼Œè¯·éªŒè¯ `invoke`ã€`ainvoke`ã€`batch`ã€`stream` æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

æµå¼ä¼ è¾“ï¼ˆå¦‚æœæ‚¨æ­£åœ¨å®ç°ï¼‰ï¼š

* [ ] å®ç° _stream æ–¹æ³•ä»¥ä½¿æµå¼ä¼ è¾“æ­£å¸¸å·¥ä½œã€‚

åœæ­¢æ ‡è®°è¡Œä¸ºï¼š

* [ ] åº”å°Šé‡åœæ­¢æ ‡è®°ã€‚
* [ ] åœæ­¢æ ‡è®°åº”ä½œä¸ºå“åº”çš„ä¸€éƒ¨åˆ†åŒ…å«åœ¨å†…ã€‚

ç§˜å¯† API å¯†é’¥ï¼š

* [ ] å¦‚æœæ‚¨çš„æ¨¡å‹è¿æ¥åˆ° APIï¼Œå®ƒå¯èƒ½ä¼šåœ¨åˆå§‹åŒ–æ—¶æ¥å— API å¯†é’¥ã€‚ä½¿ç”¨ Pydantic çš„ `SecretStr` ç±»å‹æ¥å¤„ç†ç§˜å¯†ï¼Œä»¥ä¾¿åœ¨æœ‰äººæ‰“å°æ¨¡å‹æ—¶ä¸ä¼šæ„å¤–æ‰“å°å‡ºæ¥ã€‚

è¯†åˆ«å‚æ•°ï¼š

* [ ] åœ¨è¯†åˆ«å‚æ•°ä¸­åŒ…å« `model_name`ã€‚

ä¼˜åŒ–ï¼š

è€ƒè™‘æä¾›åŸç”Ÿå¼‚æ­¥æ”¯æŒï¼Œä»¥å‡å°‘æ¨¡å‹çš„å¼€é”€ï¼

* [ ] æä¾› `_agenerate` çš„åŸç”Ÿå¼‚æ­¥æ”¯æŒï¼ˆç”¨äº `ainvoke`ï¼‰ã€‚
* [ ] æä¾› `_astream` çš„åŸç”Ÿå¼‚æ­¥æ”¯æŒï¼ˆç”¨äº `astream`ï¼‰ã€‚

## ä¸‹ä¸€æ­¥

æ‚¨ç°åœ¨å·²ç»å­¦ä¹ äº†å¦‚ä½•åˆ›å»ºè‡ªå·±çš„è‡ªå®šä¹‰èŠå¤©æ¨¡å‹ã€‚

æ¥ä¸‹æ¥ï¼Œè¯·æŸ¥çœ‹æœ¬èŠ‚ä¸­å…¶ä»–å…³äºèŠå¤©æ¨¡å‹çš„æ“ä½œæŒ‡å—ï¼Œä¾‹å¦‚ [å¦‚ä½•è®©æ¨¡å‹è¿”å›ç»“æ„åŒ–è¾“å‡º](/docs/how_to/structured_output) æˆ– [å¦‚ä½•è·Ÿè¸ªèŠå¤©æ¨¡å‹çš„ä»¤ç‰Œä½¿ç”¨æƒ…å†µ](/docs/how_to/chat_token_usage_tracking)ã€‚